{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR Reviewer Builder v2\n",
    "\n",
    "This notebook builds a dataset for few-shot learning by identifying:\n",
    "- Commit before SME review\n",
    "- Commit after SME review  \n",
    "- File paths of changes\n",
    "- SME comment text\n",
    "- Whether the after-commit actually addresses the comment\n",
    "\n",
    "The algorithms help detect whether commits after SME reviews actually respond to review comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# For fuzzy string matching\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "except ImportError:\n",
    "    print(\"rapidfuzz not installed. Installing...\")\n",
    "    !pip install rapidfuzz\n",
    "    from rapidfuzz import fuzz\n",
    "\n",
    "# For data processing\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class CodeSpan:\n",
    "    \"\"\"Represents a span of code with line numbers and content.\"\"\"\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    content: str\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class ReviewComment:\n",
    "    \"\"\"Represents an SME review comment.\"\"\"\n",
    "    comment_id: str\n",
    "    text: str\n",
    "    file_path: Optional[str] = None\n",
    "    line_number: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class CommitInfo:\n",
    "    \"\"\"Represents information about a commit.\"\"\"\n",
    "    commit_sha: str\n",
    "    message: str\n",
    "    files_changed: List[str]\n",
    "    timestamp: str\n",
    "\n",
    "class FollowThroughAdvanced:\n",
    "    \"\"\"Advanced follow-through scoring class.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def calculate_score(self, before_code: str, after_code: str, comment: str) -> float:\n",
    "        \"\"\"Calculate follow-through score based on code changes and comment.\"\"\"\n",
    "        # This will be implemented in the scoring section\n",
    "        # Implement a simple scoring algorithm based on code changes and comment relevance\n",
    "        if not before_code and not after_code:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate how much the code changed\n",
    "        edit_distance = normalized_edit_distance(before_code, after_code)\n",
    "        \n",
    "        # Look for keywords in comment that suggest the type of change expected\n",
    "        comment_lower = comment.lower()\n",
    "        action_keywords = ['fix', 'update', 'change', 'modify', 'improve', 'refactor', 'add', 'remove']\n",
    "        \n",
    "        keyword_score = 0.0\n",
    "        for keyword in action_keywords:\n",
    "            if keyword in comment_lower:\n",
    "                keyword_score += 0.1\n",
    "        \n",
    "        keyword_score = min(keyword_score, 1.0)\n",
    "        \n",
    "        # Combine edit distance with keyword relevance\n",
    "        combined_score = (edit_distance * 0.7) + (keyword_score * 0.3)\n",
    "        \n",
    "        return min(combined_score, 1.0)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Harvesting Section\n",
    "\n",
    "This section contains the existing harvesting logic that should remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def harvest_pr_data(repo_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Harvest PR data from repository.\n",
    "    \n",
    "    This function remains unchanged and handles the data collection.\n",
    "    \"\"\"\n",
    "    # Placeholder for existing harvesting logic\n",
    "    return {\n",
    "        'commits': [],\n",
    "        'reviews': [],\n",
    "        'file_changes': []\n",
    "    }\n",
    "\n",
    "def extract_code_spans(file_path: str, line_ranges: List[Tuple[int, int]]) -> List[CodeSpan]:\n",
    "    \"\"\"Extract code spans from file at specified line ranges.\n",
    "    \n",
    "    This function remains unchanged.\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "            for start, end in line_ranges:\n",
    "                content = ''.join(lines[start-1:end]) if start <= len(lines) else ''\n",
    "                spans.append(CodeSpan(start, end, content, file_path))\n",
    "    return spans"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Algorithms Section\n",
    "\n",
    "This section contains the algorithms that need to be implemented to detect whether commits after SME reviews actually respond to review comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TODO: Implement these functions\n",
    "\n",
    "def concat_snips(spans: List[CodeSpan]) -> str:\n",
    "    \"\"\"Concatenate code snippets from spans for analysis.\n",
    "    \n",
    "    Args:\n",
    "        spans: List of CodeSpan objects containing code snippets\n",
    "        \n",
    "    Returns:\n",
    "        str: Concatenated code text from all spans\n",
    "    \"\"\"\n",
    "    if not spans:\n",
    "        return \"\"\n",
    "    \n",
    "    # Sort spans by file path and then by start line for consistent ordering\n",
    "    sorted_spans = sorted(spans, key=lambda s: (s.file_path, s.start_line))\n",
    "    \n",
    "    # Concatenate content from all spans with file separators\n",
    "    result_parts = []\n",
    "    current_file = None\n",
    "    \n",
    "    for span in sorted_spans:\n",
    "        # Add file header if we're switching to a new file\n",
    "        if current_file != span.file_path:\n",
    "            if current_file is not None:\n",
    "                result_parts.append(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "            result_parts.append(f\"# File: {span.file_path}\\\\n\")\n",
    "            current_file = span.file_path\n",
    "        \n",
    "        # Add line number information and content\n",
    "        result_parts.append(f\"# Lines {span.start_line}-{span.end_line}:\\\\n\")\n",
    "        result_parts.append(span.content)\n",
    "        \n",
    "        # Add separator between spans in same file\n",
    "        if not span.content.endswith(\"\\\\n\"):\n",
    "            result_parts.append(\"\\\\n\")\n",
    "        result_parts.append(\"\\\\n\")\n",
    "    \n",
    "    return \"\".join(result_parts).strip()\n",
    "\n",
    "\n",
    "def normalized_edit_distance(before_code: str, after_code: str) -> float:\n",
    "    \"\"\"Calculate normalized edit distance between before/after code using fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        before_code: Code before the change\n",
    "        after_code: Code after the change\n",
    "        \n",
    "    Returns:\n",
    "        float: Normalized edit distance (0.0 = identical, 1.0 = completely different)\n",
    "    \"\"\"\n",
    "    if not before_code and not after_code:\n",
    "        return 0.0\n",
    "    \n",
    "    if not before_code or not after_code:\n",
    "        return 1.0\n",
    "    \n",
    "    # Normalize whitespace for better comparison\n",
    "    before_normalized = ' '.join(before_code.split())\n",
    "    after_normalized = ' '.join(after_code.split())\n",
    "    \n",
    "    # Calculate similarity ratio using rapidfuzz\n",
    "    similarity = fuzz.ratio(before_normalized, after_normalized) / 100.0\n",
    "    \n",
    "    # Return distance (1 - similarity)\n",
    "    return 1.0 - similarity\n",
    "\n",
    "\n",
    "def simple_locality_score(comment_text: str, file_paths: List[str]) -> float:\n",
    "    \"\"\"Compute locality scores based on file path mentions in comments.\n",
    "    \n",
    "    Args:\n",
    "        comment_text: The review comment text\n",
    "        file_paths: List of file paths that were changed\n",
    "        \n",
    "    Returns:\n",
    "        float: Locality score (0.0 = no file path mentions, 1.0 = all files mentioned)\n",
    "    \"\"\"\n",
    "    if not comment_text or not file_paths:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize comment text to lowercase for case-insensitive matching\n",
    "    comment_lower = comment_text.lower()\n",
    "    \n",
    "    # Count how many file paths are mentioned in the comment\n",
    "    mentioned_files = 0\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Check various forms of the file path\n",
    "        file_name = file_path.split('/')[-1]  # Just filename\n",
    "        file_path_lower = file_path.lower()\n",
    "        file_name_lower = file_name.lower()\n",
    "        \n",
    "        # Check if file path or filename is mentioned\n",
    "        if (file_path_lower in comment_lower or \n",
    "            file_name_lower in comment_lower or\n",
    "            # Also check without extension\n",
    "            file_name_lower.split('.')[0] in comment_lower):\n",
    "            mentioned_files += 1\n",
    "    \n",
    "    # Return ratio of mentioned files to total files\n",
    "    return mentioned_files / len(file_paths)\n",
    "\n",
    "\n",
    "def score_follow_through(before_commit: CommitInfo, \n",
    "                        after_commit: CommitInfo,\n",
    "                        review_comment: ReviewComment,\n",
    "                        before_spans: List[CodeSpan],\n",
    "                        after_spans: List[CodeSpan]) -> Dict[str, float]:\n",
    "    \"\"\"Score follow-through using the FollowThroughAdvanced class with proper threshold handling.\n",
    "    \n",
    "    Args:\n",
    "        before_commit: Commit information before SME review\n",
    "        after_commit: Commit information after SME review\n",
    "        review_comment: SME review comment\n",
    "        before_spans: Code spans before the change\n",
    "        after_spans: Code spans after the change\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary containing various scores:\n",
    "            - 'edit_distance': Normalized edit distance between code\n",
    "            - 'locality_score': How well comment matches changed files\n",
    "            - 'follow_through_score': Overall follow-through score\n",
    "            - 'addresses_comment': Binary score (0/1) if comment is addressed\n",
    "    \"\"\"\n",
    "    # Concatenate code snippets for comparison\n",
    "    before_code = concat_snips(before_spans)\n",
    "    after_code = concat_snips(after_spans)\n",
    "    \n",
    "    # Calculate normalized edit distance\n",
    "    edit_distance = normalized_edit_distance(before_code, after_code)\n",
    "    \n",
    "    # Calculate locality score\n",
    "    all_file_paths = list(set(after_commit.files_changed + before_commit.files_changed))\n",
    "    locality_score = simple_locality_score(review_comment.text, all_file_paths)\n",
    "    \n",
    "    # Use FollowThroughAdvanced to calculate overall score\n",
    "    follow_through = FollowThroughAdvanced(threshold=0.5)\n",
    "    follow_through_score = follow_through.calculate_score(\n",
    "        before_code, after_code, review_comment.text\n",
    "    )\n",
    "    \n",
    "    # Calculate if comment is addressed based on combined factors\n",
    "    # High edit distance (changes were made) + high locality (relevant files) = good follow-through\n",
    "    change_significance = min(edit_distance * 2, 1.0)  # Scale edit distance\n",
    "    combined_score = (change_significance * 0.4 + locality_score * 0.3 + \n",
    "                     (follow_through_score or 0.5) * 0.3)\n",
    "    \n",
    "    addresses_comment = 1.0 if combined_score > 0.5 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'edit_distance': edit_distance,\n",
    "        'locality_score': locality_score,\n",
    "        'follow_through_score': follow_through_score or 0.5,\n",
    "        'addresses_comment': addresses_comment\n",
    "    }\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Section\n",
    "\n",
    "This section contains the existing export logic that should remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def export_dataset(data: Dict[str, Any], output_path: str) -> None:\n",
    "    \"\"\"Export the processed dataset to file.\n",
    "    \n",
    "    This function remains unchanged and handles the data export.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Dataset exported to {output_path}\")\n",
    "\n",
    "def create_few_shot_examples(scored_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Create few-shot learning examples from scored data.\n",
    "    \n",
    "    This function remains unchanged.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for item in scored_data:\n",
    "        if item.get('addresses_comment', 0) > 0.5:  # Threshold for positive examples\n",
    "            examples.append({\n",
    "                'before_code': item.get('before_code', ''),\n",
    "                'after_code': item.get('after_code', ''),\n",
    "                'comment': item.get('comment_text', ''),\n",
    "                'label': 'addresses_comment'\n",
    "            })\n",
    "    return examples"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "This section orchestrates the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main_pipeline(repo_path: str, output_path: str) -> None:\n",
    "    \"\"\"Main pipeline to process PR data and build dataset.\"\"\"\n",
    "    print(\"Starting PR Reviewer dataset building...\")\n",
    "    \n",
    "    # Step 1: Harvest data (unchanged)\n",
    "    print(\"Harvesting PR data...\")\n",
    "    pr_data = harvest_pr_data(repo_path)\n",
    "    \n",
    "    # Step 2: Process and score data using implemented algorithms\n",
    "    print(\"Processing and scoring data...\")\n",
    "    scored_results = []\n",
    "    \n",
    "    # This would iterate through the harvested data and apply scoring\n",
    "    # Implementation details depend on the harvested data structure\n",
    "    \n",
    "    # Step 3: Export results (unchanged)\n",
    "    print(\"Exporting dataset...\")\n",
    "    export_dataset({\n",
    "        'scored_results': scored_results,\n",
    "        'metadata': {\n",
    "            'total_items': len(scored_results),\n",
    "            'repo_path': repo_path\n",
    "        }\n",
    "    }, output_path)\n",
    "    \n",
    "    print(\"Dataset building complete!\")\n",
    "\n",
    "# Example usage (commented out)\n",
    "# main_pipeline('/path/to/repo', 'pr_reviewer_dataset.json')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Section\n",
    "\n",
    "This section will be used to test the implemented functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the implemented functions once they are completed\n",
    "def test_algorithms():\n",
    "    \"\"\"Test the implemented algorithm functions.\"\"\"\n",
    "    print(\"Testing implemented algorithms...\")\n",
    "    \n",
    "    # Test data\n",
    "    test_spans = [\n",
    "        CodeSpan(1, 3, \"def hello():\\n    print('Hello')\\n    return True\", \"test.py\"),\n",
    "        CodeSpan(5, 7, \"def world():\\n    print('World')\\n    return False\", \"test.py\")\n",
    "    ]\n",
    "    \n",
    "    # Test concat_snips\n",
    "    print(\"Testing concat_snips...\")\n",
    "    result = concat_snips(test_spans)\n",
    "    # print(f\"Concatenated result: {result[:100]}...\")  # First 100 chars\n",
    "    \n",
    "    # Test normalized_edit_distance\n",
    "    print(\"Testing normalized_edit_distance...\")\n",
    "    distance = normalized_edit_distance(\"hello world\", \"hello earth\")\n",
    "    # print(f\"Edit distance: {distance}\")\n",
    "    \n",
    "    # Test simple_locality_score\n",
    "    print(\"Testing simple_locality_score...\")\n",
    "    score = simple_locality_score(\"Please fix the issue in test.py\", [\"test.py\", \"main.py\"])\n",
    "    # print(f\"Locality score: {score}\")\n",
    "    \n",
    "    print(\"Algorithm testing complete!\")\n",
    "\n",
    "    # Functions are now implemented, ready to test\n",
    "test_algorithms()\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}